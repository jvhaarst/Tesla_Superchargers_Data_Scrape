{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping current supercharger data from Teslas website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This uses the complete width of the screen with Jupyter\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin,urlparse,parse_qs\n",
    "import re\n",
    "from datetime import datetime, timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of countries that have Superchargers\n",
    "url = 'https://www.tesla.com/findus/list'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content)\n",
    "countries = []\n",
    "for row in soup.find_all('section', {'class': 'row'}):\n",
    "    for h2 in row.find_all('h2'):\n",
    "        if 'Superchargers' in (h2.text):\n",
    "            hrefs = row.find_all('a', href=True)\n",
    "            for href in hrefs:\n",
    "                country = href['href'].rsplit('/')[-1]\n",
    "                countries.append(country)\n",
    "countries = list(set(countries))\n",
    "countries.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to grab locations of Superchargers from country website\n",
    "def get_locations(url,country):\n",
    "    locations = []\n",
    "    response = requests.get(url)\n",
    "    try:\n",
    "        soup = BeautifulSoup(response.content, 'lxml')\n",
    "        if soup:\n",
    "            for address in soup.find_all('address', {'class': 'vcard'}):\n",
    "                location = {}\n",
    "                location['URL'] = urljoin(url ,address.find('a', href=True)['href'])\n",
    "                # Some locations do not have an address, so we skip if this fails\n",
    "                try:\n",
    "                    location['street_address'] = address.find('span', {'class': 'street-address'}).text.strip()\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    location['locality'] = address.find('span', {'class': 'locality'}).text.strip()\n",
    "                except:\n",
    "                    pass\n",
    "                locations.append(location)\n",
    "    except:\n",
    "        print(\"Fail on {}\".format(country))\n",
    "        pass\n",
    "    return locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = []\n",
    "# Show progressbar\n",
    "from tqdm.notebook import tqdm\n",
    "# Loop over countries, and store info in list of dicts\n",
    "for country in tqdm(set(countries)):\n",
    "    url = 'https://www.tesla.com/findus/list/superchargers/'\n",
    "    url = urljoin(url, country)\n",
    "    locations = locations + get_locations(url, country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pandas dataframe from locations\n",
    "df = pd.DataFrame(locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove double whitespaces from dataframe\n",
    "df = df.replace(to_replace ='\\s+', value = ' ', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some info on dataframe\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store intermediate result, so we have it if the notebook fails\n",
    "filename = 'df.'+datetime.utcnow().strftime('%Y%m%d')+'.parquet'\n",
    "df.to_parquet(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract info from location websites\n",
    "def get_info_from_url(url, index, debug=False):\n",
    "    # Use the global datframe, so we do not have to shuffle data around\n",
    "    global df\n",
    "    # Grab data from URL\n",
    "    response = requests.get(url)\n",
    "    # We use the lxml parser, as the html parser gave weird results (&center -> Â¢er)\n",
    "    soup = BeautifulSoup(response.content, 'lxml')\n",
    "    # Try to grab coordinate info\n",
    "    # Do we have a \"Driving Directions\" entry ?\n",
    "    hrefs = soup.find_all('a', href=True)\n",
    "    coords = ''\n",
    "    for href in hrefs:\n",
    "        if href.text == 'Driving Directions':\n",
    "            coords = urlparse(href['href']).query.split('=')[1]\n",
    "    # If not, use grab the info from the map image\n",
    "    if len(coords) == 0:\n",
    "        div = soup.find('div', {'id': 'location-map'})\n",
    "        if div:\n",
    "            img_src = div.find('img')['src']\n",
    "            if img_src:\n",
    "                coords = parse_qs(urlparse(img_src).query)['center'][0]\n",
    "    # Add coordinate info to dataframe, leave empty if missing\n",
    "    if coords:\n",
    "        df.loc[index, 'latitude'] = coords.split(',')[0]\n",
    "        df.loc[index, 'longitude'] = coords.split(',')[1]\n",
    "    else:\n",
    "        df.loc[index, 'latitude'] = ''\n",
    "        df.loc[index, 'longitude'] = ''\n",
    "    # Grab charging info from \"Charging\" line\n",
    "    charging_text = soup.select_one('p:-soup-contains(\"Charging\")')\n",
    "    if charging_text:\n",
    "        charging_text= charging_text.get_text(separator=\" \")\n",
    "        # Store text for later , better parsing\n",
    "        df.loc[index, 'charging_text'] = charging_text\n",
    "        if debug:\n",
    "            print(charging_text)\n",
    "        # Grab first info on charging\n",
    "        number = re.search('(\\d+).*Superchargers', charging_text)\n",
    "        if number:\n",
    "            number = number.group(1)\n",
    "            df.loc[index, 'number'] = number\n",
    "            if debug:\n",
    "                print(number)\n",
    "        kw = re.search('(\\d+)kW', charging_text)\n",
    "        if kw:\n",
    "            kw = kw.group(1)        \n",
    "            df.loc[index, 'kw'] = kw\n",
    "            if debug:\n",
    "                print(kw)        \n",
    "    else:\n",
    "        df.loc[index, 'number'] = ''\n",
    "        df.loc[index, 'kw'] = ''\n",
    "            \n",
    "    # Grab info on possibility of charging for Non-Tesla cars\n",
    "    open_text = soup.select_one('i:-soup-contains(\"Non-Tesla\")')\n",
    "    if open_text:\n",
    "        df.loc[index, 'open'] = open_text.get_text(separator=\" \")\n",
    "        if debug:\n",
    "            print(open_text)\n",
    "    else:\n",
    "        df.loc[index, 'open'] = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import traceback\n",
    "# Loop over all locations, and grab extra info from URL (and show nicer errors on fail)\n",
    "for index,location in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    try:\n",
    "        get_info_from_url(location['URL'],index)\n",
    "    except Exception:\n",
    "        print(\"Fail on ({}):{}\".format(index,location['URL']))\n",
    "        traceback.print_exc()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store intermediate result, so we have it if the notebook fails\n",
    "filename = 'df.filled.'+datetime.utcnow().strftime('%Y%m%d')+'.parquet'\n",
    "df.to_parquet(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does the datafram look like now ?\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we parse the charging text, and grab all kW values and number of stalls.\n",
    "from tqdm.notebook import tqdm\n",
    "import traceback\n",
    "for index,location in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    charging_text = location[\"charging_text\"]\n",
    "    try:\n",
    "        if charging_text and isinstance(charging_text, (str, bytes)):\n",
    "            counts = re.findall('(\\d+)\\s+Superchargers',charging_text)\n",
    "            powers = re.findall('(\\d+)kW',charging_text)\n",
    "            # For each pair of kW and stall count, add the kW value as column, and count as value\n",
    "            for count, power in zip(counts,powers):\n",
    "                df.loc[index, power] = count\n",
    "    except:\n",
    "        print(index,charging_text, type(charging_text))\n",
    "        traceback.print_exc()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    if column.isdigit():\n",
    "        df[column]= pd.to_numeric(df[column], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store final result\n",
    "filename = 'df.done.'+datetime.utcnow().strftime('%Y%m%d')+'.parquet'\n",
    "df.to_parquet(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can show the complete result.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[(df['150'] > 0)& (df.open.str.contains('Non'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[(df['150'] > 0)& (df.open.str.contains('Non'))].to_csv(\"150_world_open.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
